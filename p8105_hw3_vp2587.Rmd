---
title: "P8105 HW3"
author: "Veerapetch Petchger"
date: "2025-10-05"
output: github_document
---
## Problem 1
```{r setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)
library(patchwork)
data("instacart")
```
The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illustrative examples of observations. Then, do or answer the following (commenting on the results of each):

1. How many aisles are there, and which aisles are the most items ordered from?
```{r most_items}
instacart %>% 
  count(aisle, name = "n_obs") %>%
  arrange(desc(n_obs)) %>% 
  head()
```

There are `r instacart %>% summarise(n_aisles = n_distinct(aisle))` different aisles. The `fresh vegetables` and `fresh fruits` are by far the most popular aisles, followed by `packaged vegetables fruits`, `yogurt`, and `packaged cheese`.

2. Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.
```{r 10000}
instacart %>%
  count(aisle, name = "n_obs") %>%
  filter(n_obs > 10000) %>%
  mutate(aisle = forcats::fct_reorder(aisle, n_obs)) %>%
  ggplot(aes(x = aisle, y = n_obs, fill = aisle)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Aisles With Over 10000 Items Ordered",
    x = "Aisle Names",
    y = "Number of of Items Ordered"
  ) +
  viridis::scale_color_viridis(discrete = TRUE) +
  theme_minimal() +
  theme(legend.position = "none")
```

3. Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r top_3}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(order_count = n()) %>%
  arrange(aisle, desc(order_count)) %>%
  slice_max(order_count, n = 3) %>% 
  knitr::kable(caption = "Top 3 Most Popular Items from the Baking Ingredients, Dog Food Care, and Packaged Vegetables/Fruits Aisles")
```

4. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).
```{r mean_hour}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour,
    names_prefix = "Day_"
  ) %>% 
    knitr::kable(caption = "Average Hour of Ordering Pink Lady Apples and Coffe Ice Cream by Day of the Week")
```

## Problem 2
This Problem uses the Zillow datasets introduced in Homework 2. Both datasets are available here. Import, clean, and otherwise tidy these datasets.
```{r zillow}
zip_codes_df =
  read_csv("zillow_data/Zip Codes.csv", na = c("NA",".","")) %>% 
  janitor::clean_names() %>% 
  mutate(
    zip_code = as.character(zip_code),
    borough  = recode(
      county,
      "New York" = "Manhattan",
      "Kings"    = "Brooklyn",
      "Queens"   = "Queens",
      "Bronx"    = "Bronx",
      "Richmond" = "Staten Island",
    )
  ) %>%
  select(zip_code, borough)

zora_df =
  read_csv("zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv", na = c("NA",".","")) %>% 
  janitor::clean_names() %>%
  pivot_longer(
    cols = x2015_01_31:x2024_08_31,
    names_to = "ym_str",
    values_to = "rent"
  ) %>%
  rename(zip_code = region_name) %>% 
  mutate(
    ym_str = stringr::str_remove(ym_str, "^x"),
    ym_str = stringr::str_replace_all(ym_str, "_", "-"),
    date   = lubridate::ymd(ym_str),
    year   = lubridate::year(date),
    month  = lubridate::month(date),
    day    = lubridate::day(date),
    zip_code = as.character(zip_code)) %>%
  select(-ym_str)

zillow_df =
  zora_df %>% 
  left_join(zip_codes_df, by = "zip_code")
```

1. There are 116 months between January 2015 and August 2024. How many ZIP codes are observed 116 times? How many are observed fewer than 10 times? Why are some ZIP codes are observed rarely and others observed in each month?
```{r zip_count}
zip_count_df =
  zillow_df %>% 
  filter(
    date >= "2015-01-01" & date <= "2025-08-01",
    !is.na(zip_code),
    !is.na(rent)
         ) %>% 
  group_by(zip_code) %>% 
  summarize(n_obs = n_distinct(date))
```
There are `r zip_count_df %>% filter(n_obs == 116) %>% nrow()` ZIP codes observed for all the months, and `r zip_count_df %>% filter(n_obs < 10) %>% nrow()` ZIP codes observed fewer than 10 times. The reasons for the difference in coverage across ZIP codes could be that 1) months with very few listings are not published, 2) ZIP codes may have been introduced, discontinued, or redefined during the time period, and 3) ZIP codes that are primarily non-residential such as airports or campuses will have inconsistent rental data.

2. Create a reader-friendly table showing the average rental price in each borough and year (not month). Comment on trends in this table.
```{r borough_year}
zillow_df %>%
  filter(
    !is.na(zip_code),
    !is.na(borough),
    !is.na(rent)) %>% 
  group_by(borough, year) %>% 
  summarize(avg_rent = mean(rent, na.rm = TRUE)) %>% 
  arrange(year) %>% 
  pivot_wider(
    names_from = "borough",
    values_from = "avg_rent"
  ) %>% 
  knitr::kable(caption = "Average ZORI by Borough and Year")
```
Apart from Staten Island, there is a general increase in rent from 2015-2019. The pandemic caused different reactions across the boroughs in 2020-2021, after which average rent prices continued to steadily increase. Manhattan is visibly the most expensive borough, followed by Brooklyn, Queens, Staten Island, and the Bronx.

3. Make a plot showing NYC Rental Prices within ZIP codes for all available years. Your plot should facilitate comparisons across boroughs. Comment on any significant elements of this plot.
```{r rent_p}
rent_p =  
  zillow_df %>% 
  filter(
    !is.na(zip_code),
    !is.na(borough),
    !is.na(rent)) %>% 
  ggplot(aes(x = date, y = rent, group = zip_code, color = borough)) +
  geom_line(linewidth = 0.1) +
  labs(
    title = "NYC Rental Prices by ZIP Code from 2015–2024",
    x = "Year",
    y = "Rental Price",
    color = "Borough"
  ) +
  scale_y_continuous(labels = scales::dollar) +
  viridis::scale_color_viridis(discrete = TRUE) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none") +
  facet_wrap(~ borough, ncol = 3)
rent_p
```

This figure depicts rental price trends of every NYC ZIP code from 2015-2024, sorted by borough. Each line represents one ZIP code, allowing for comparisons within boroughs. These findings reinforce the previous statements regarding the pandemic's disruptions and subsequent rebounding of the city's rental market. Manhattan and Brooklyn, the most expensive boroughs, demonstrated higher volatility compared to the other boroughs.

4. Compute the average rental price within each ZIP code over each month in 2023. Make a reader-friendly plot showing the distribution of ZIP-code-level rental prices across boroughs; put differently, your plot should facilitate the comparison of the distribution of average rental prices across boroughs. Comment on this plot.
```{r monthly_avg_2023_p}
monthly_avg_2023_p =
  zillow_df %>% 
  filter(
    year == 2023,
    !is.na(zip_code),
    !is.na(borough),
    !is.na(rent)
  ) %>%
  group_by(borough, zip_code) %>% 
  summarize(monthly_avg_2023 = mean(rent, na.rm = TRUE), .groups = "drop") %>%
  ggplot(aes(x = borough, y = monthly_avg_2023, fill = borough)) +
  geom_boxplot(width = 0.5) +
  scale_y_continuous(labels = scales::dollar) +
  labs(
    title = "Average Monthly NYC Rental Prices by ZIP Code in 2023",
    x = "Borough",
    y = "Rental Price",
    color = "Borough"
  ) +
  viridis::scale_fill_viridis(discrete = TRUE) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none")
monthly_avg_2023_p
```
This plot depicts the distribution of the average ZIP-code-level monthly rent across boroughs in 2023. Manhattan has the highest median, but also the widest spread of data. Brooklyn's median is the second largest and exhibits a similar spread of data to that of Manhattan. The remaining boroughs have lower and similar medians and spread of data. Manhattan and Queens have outlying data points, corresponding to their luxury submarkets.

5. Combine the two previous plots into a single graphic, and export this to a results folder in your repository.

```{r combined}
monthly_avg_2023_p / rent_p
ggsave("combined.pdf", height = 8)           
```

## Problem 3
Accelerometers have become an appealing alternative to self-report techniques for studying physical activity in observational studies and clinical trials, largely because of their relative objectivity. During observation periods, the devices can measure MIMS in a short period; one-minute intervals are common. Because accelerometers can be worn comfortably and unobtrusively, they produce around-the-clock observations.

1) Load, tidy, merge, and otherwise organize the data sets. Your final dataset should include all originally observed variables; exclude participants less than 21 years of age, and those with missing demographic data; and encode data with reasonable variable classes (i.e. not numeric, and using factors with the ordering of tables and plots in mind).
```{r nhanes}
demographic_df =
  read_csv("nhanes_covar.csv", na = c("NA", ".", ""), skip = 4) %>% 
  janitor::clean_names() %>% 
  filter(age >= 21) %>% 
  mutate(
    sex = recode(sex, "1" = "Male", "2" = "Female"),
    education = recode(education,
                       "1" = "Less than high school",
                       "2" = "High school equivalent",
                       "3" = "More than high school"),
    sex = factor(sex, levels = c("Male","Female")),
    education = factor(
      education,
      levels = c("Less than high school","High school equivalent","More than high school"),
      ordered = TRUE
  )
  ) %>% 
    drop_na()

accelerometer_df =
  read_csv("nhanes_accel.csv", na = c("NA", ".", "")) %>% 
  janitor::clean_names()

nhanes_df = 
  left_join(demographic_df, accelerometer_df, by = "seqn")
```

2) Produce a reader-friendly table for the number of men and women in each education category, and create a visualization of the age distributions for men and women in each education category. Comment on these items.
```{r age_count}
nhanes_df %>% 
  count(education, sex) %>% 
  pivot_wider(
    names_from = sex,
    values_from = n
  ) %>% 
  knitr::kable(caption = "Number of Males and Females in Each Education Category")

nhanes_df %>% 
  ggplot(aes(x = age, color = sex, fill = sex)) +
  geom_histogram(position = "dodge", binwidth = 5) +
  labs(
    title = "Number of Males and Females in Each Education Category",
    x = "Sex",
    y = "Count"
  ) +
  viridis::scale_fill_viridis(discrete = TRUE) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  facet_grid(sex ~ education)
```

3) Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate across minutes to create a total activity variable for each participant. Plot these total activities (y-axis) against age (x-axis); your plot should compare men to women and have separate panels for each education level. Include a trend line or a smooth to illustrate differences. Comment on your plot.
```{r mims_age}
nhanes_df %>% 
  mutate(
    total_mims = rowSums(across(starts_with("min")), na.rm = TRUE)) %>%
  select(-starts_with("min")) %>% 
  ggplot(aes(x = age, y = total_mims, color = sex)) +
  labs(
    title = "Total Daily Activity vs Age",
    x = "Age",
    y = "Total MIMS",
    color = "Sex"
  ) +
  geom_point() +
  geom_smooth(se = FALSE) + 
  viridis::scale_color_viridis(discrete = TRUE) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  facet_wrap(~ education)
```

4) Accelerometer data allows the inspection activity over the course of the day. Make a three-panel plot that shows the 24-hour activity time courses for each education level and use color to indicate sex. Describe in words any patterns or conclusions you can make based on this graph; including smooth trends may help identify differences.
```{r mims_24_hour}
nhanes_df %>% 
  pivot_longer(
    cols = starts_with("min"),
    names_to = "minute",
    values_to = "mims") %>% 
  mutate(minute = row_number(), .by = seqn) %>% 
  group_by(education, sex, minute) %>% 
  summarize(
    mean_mims = mean(mims, na.rm = TRUE), .groups = "drop"
  ) %>% 
  ggplot(aes(x = minute, y = mean_mims, color = sex)) +
  scale_x_continuous(
    breaks = c(0, 360, 720, 1080, 1440),
    labels = c("00:00", "06:00", "12:00", "18:00", "24:00")
  ) +
  labs(
    title = "24-Hour Activity Profiles by Education Level",
    x = "Minute",
    y = "Mean Activity (MIMS)"
  ) +
  geom_line(linewidth = 0.1) +
  geom_smooth(se = FALSE, linewidth = 0.5) +
  viridis::scale_color_viridis(discrete = TRUE) +
  theme_minimal() +
  theme(legend.position = "bottom",
        panel.spacing = unit(20, "pt")) +
  facet_wrap(~ education)
```
